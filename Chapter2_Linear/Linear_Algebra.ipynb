{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "x+y, x*y, x/y, x**y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, torch.Size([4]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x),x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape((5,4))\n",
    "A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='orange'>矩阵的转置</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape((2,3,4))\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()\n",
    "A, B, A+B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B ##两个矩阵的按元素乘法，称为哈达玛积，并非矩阵乘法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0,  1,  2,  3],\n          [ 4,  5,  6,  7],\n          [ 8,  9, 10, 11]],\n \n         [[12, 13, 14, 15],\n          [16, 17, 18, 19],\n          [20, 21, 22, 23]]]),\n tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n tensor([[[ 0,  2,  4,  6],\n          [ 8, 10, 12, 14],\n          [16, 18, 20, 22]],\n \n         [[24, 26, 28, 30],\n          [32, 34, 36, 38],\n          [40, 42, 44, 46]]]),\n torch.Size([2, 3, 4]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X, a + X, a*X, (a*X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor(6.))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x, x.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.],\n          [16., 17., 18., 19.]],\n \n         [[20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.],\n          [32., 33., 34., 35.],\n          [36., 37., 38., 39.]]]),\n torch.Size([2, 5, 4]),\n tensor(780.))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20*2, dtype=torch.float32).reshape(2,5,4)\n",
    "A, A.shape,A.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[20., 22., 24., 26.],\n         [28., 30., 32., 34.],\n         [36., 38., 40., 42.],\n         [44., 46., 48., 50.],\n         [52., 54., 56., 58.]]),\n torch.Size([5, 4]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0) ## axis=0,1,2代表reshape(x,y,z)中的x,y,z求和\n",
    "A_sum_axis0,A_sum_axis0.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 40.,  45.,  50.,  55.],\n         [140., 145., 150., 155.]]),\n torch.Size([2, 4]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([180., 190., 200., 210.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(19.5000), tensor(19.5000))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum()/A.numel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[10., 11., 12., 13.],\n         [14., 15., 16., 17.],\n         [18., 19., 20., 21.],\n         [22., 23., 24., 25.],\n         [26., 27., 28., 29.]]),\n tensor([[10., 11., 12., 13.],\n         [14., 15., 16., 17.],\n         [18., 19., 20., 21.],\n         [22., 23., 24., 25.],\n         [26., 27., 28., 29.]]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0)/A.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='orange'> Keepdims=True在计算总和和均值时保持轴数不变这里就会将原来的[2,5,4]变为[2,1,4]</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 40.,  45.,  50.,  55.]],\n \n         [[140., 145., 150., 155.]]]),\n torch.Size([2, 1, 4]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True) ## Keepdims=True在计算总和和均值时保持轴数不变这里就会将原来的[2,5,4]变为[2,1,4]\n",
    "sum_A, sum_A.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0000, 0.0222, 0.0400, 0.0545],\n         [0.1000, 0.1111, 0.1200, 0.1273],\n         [0.2000, 0.2000, 0.2000, 0.2000],\n         [0.3000, 0.2889, 0.2800, 0.2727],\n         [0.4000, 0.3778, 0.3600, 0.3455]],\n\n        [[0.1429, 0.1448, 0.1467, 0.1484],\n         [0.1714, 0.1724, 0.1733, 0.1742],\n         [0.2000, 0.2000, 0.2000, 0.2000],\n         [0.2286, 0.2276, 0.2267, 0.2258],\n         [0.2571, 0.2552, 0.2533, 0.2516]]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='orange'>cumsum是累加求和的意思</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[  0.,   1.,   2.,   3.],\n         [  4.,   6.,   8.,  10.],\n         [ 12.,  15.,  18.,  21.],\n         [ 24.,  28.,  32.,  36.],\n         [ 40.,  45.,  50.,  55.]],\n\n        [[ 20.,  21.,  22.,  23.],\n         [ 44.,  46.,  48.,  50.],\n         [ 72.,  75.,  78.,  81.],\n         [104., 108., 112., 116.],\n         [140., 145., 150., 155.]]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=1) ## cumsum是累加求和的意思"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* <font color='orange'>torch.dot表示的向量之间的点积，仅仅适用于两边都是一维数据</font>\n",
    "* <font color='orange'>torch.mv表示的矩阵和向量之间的点积，左边为$m*n$的数据，右边需要为$n*1$的数据</font>\n",
    "* <font color='orange'>torch.mm表示矩阵和矩阵之间的点积，也就是真正意义上的矩阵乘法</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0, 1],\n         [2, 3]]),\n tensor([[0, 1],\n         [2, 3]]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x,y) ## dot是进行点积操作，点积是相同位置的按元素乘积的和"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x*y) ## 点积可以通过先按元素乘法，在进行求和来表示两个向量的点积"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 4]), torch.Size([4]), tensor([14.]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(4, dtype=torch.float32).reshape(1,4)\n",
    "A.shape, x.shape,torch.mv(A,x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(6).reshape(3,2)\n",
    "n = torch.arange(6).reshape(2,3)\n",
    "torch.mm(m,n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color='orange'>torch.norm()表示向量或矩阵的范数，及所有元素平方和的开根号</font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 3., -4.]]),\n tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.]],\n \n         [[10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.]]]),\n tensor(5.),\n tensor(49.6991))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[3.0,-4.0]])\n",
    "A = torch.arange(20,dtype=torch.float32).reshape(2,2,5)\n",
    "u, A, torch.norm(u),torch.norm(A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 3., -4.]]), tensor(7.))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, torch.abs(u).sum() ## abs是绝对值的意思"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\partial$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2., 3.])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(28., grad_fn=<MulBackward0>)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x,x)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过利用反向传播函数来自动计算y关于x每个分量的梯度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.,  5.,  9., 13.])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() ## 这里y=2*x*x\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1., 1.])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，pytorch会累积梯度，所以需要清楚之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6.])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward() ## 因为这里的y是标量，所以需要y.sum()进行求导\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u =  y.detach() ## 这里相当于将y变为一个关于x的常量，常数，赋值给u\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(-1.3753, requires_grad=True),\n tensor(-140826.0781, grad_fn=<MulBackward0>),\n tensor(102400.),\n tensor(True))"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a, d, a.grad, a.grad == d/a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}