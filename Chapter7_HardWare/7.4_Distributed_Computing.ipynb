{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 同步SGD\n",
    "> * 这里每个worker都是同步计算一个批量，称为同步SGD\n",
    "> * 假设有n个GPU，每个GPU每次处理b个样本，那么同步SGD等价于在单GPU运行批量大小为nb的SGD\n",
    "> * 在理想情况下，n个GPU可以得到相对于单GPU的n倍加速"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 性能\n",
    "> * $t_1=$在单个GPU上计算b个样本梯度的时间\n",
    "> * 假设有m个参数，一个worker每次发送和接受m个参数、梯度\n",
    ">> * $t_2=$发送和接受时间\n",
    "> * 每个批量的计算时间为$max(t_1,t_2)$\n",
    ">> * 选取足够大的b使得$t_1>t_2$\n",
    ">> * 增加b或n导致更大的批量大小， 到只需要更多计算来得到给定的模型精度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实践时的建议\n",
    "> * 使用一个大数据集\n",
    "> * 需要好的GPU-GPU的和机器-机器带宽\n",
    "> * 高效的数据读取和预处理\n",
    "> * 模型需要好的计算（FLOP）通讯（model size）比\n",
    ">> * Inception > ResNet > AlexNet\n",
    "> * 使用足够大的批量大小来得到好的系统性能\n",
    "> * 使用高效的优化算法对应大批量大小"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 总结\n",
    "> * 分布式同步数据并行是多GPU数据并行在多机器上的拓展\n",
    "> * 网络通讯通常是瓶颈\n",
    "> * 需要注意使用特别大的批量大小时收敛效率\n",
    "> * 更复杂的分布式有异步、模型并行"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}