{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 优化问题\n",
    "> * 一般形式：$$minimize \\quad f(x),subject \\quad to \\quad x\\in C$$\n",
    "> * 目标函数$f:\\mathbb{R}^n \\to \\mathbb{R}$\n",
    "> * 限制集合例子：$$C=\\{x|h_1(x)=0,...,h_m(x)=0,g_1(x)\\leqslant 0,...,g_r(x)\\leqslant 0\\}$$\n",
    "> * 如果$C=\\mathbb{R}^n$那就是不受限"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 局部最小VS全局最小\n",
    "> * 全局最小$x^*:f(x^*)\\leqslant f(x),\\forall x \\in C$\n",
    "> * 局部最小$x^*:$存在$\\varepsilon$，使得$f(x^*)\\leqslant f(x),\\forall x:||x-x^*||\\leqslant \\varepsilon$\n",
    "> * 使用迭代优化算法来求解，一般只能保证找到局部局部最小值\n",
    "> ![](../images/Local-minimumVSGlobal-minimum.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 凸集\n",
    "> * 一个$\\mathbb{R}^n$的子集C是凸当且仅当$$\\alpha x+(1-\\alpha)y\\in C$$$$\\forall\\alpha\\in[0,1]\\quad\\forall x, y\\in C$$\n",
    "> ![](../images/Convex-Collection.png)\n",
    "> * 左边是凸集，右边不是凸集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 凸函数\n",
    "> * 函数$f:C\\to\\mathbb{R}$是凸，当且仅当$f(\\alpha x + (1-\\alpha)y)\\leqslant \\alpha f(x) + (1-\\alpha)f(y)\\quad \\forall \\alpha\\in[0,1]\\quad \\forall x,y\\in C $\n",
    "> * 如果$x\\ne y,\\alpha\\in(0,1)$时不等式严格成立，那么叫严格凸函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 凸函数优化\n",
    "> * 如果代价函数$f$是凸的，且限制集合$C$是凸的，那么就是凸优化问题，那么局部最小一定是全局最小\n",
    "> * 严格凸优化问题有唯一的全局最小\n",
    "> ![](../images/Convex-Function-Optimization.png)\n",
    "> 左边不是严格凸函数，所以他存在很多的全局最小"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 凸和非凸例子\n",
    "> * 凸\n",
    ">> * 线性回归$f(x)=||Wx-b||_2^2$\n",
    ">> * Softmax回归\n",
    "> * 非凸:其它\n",
    ">> * MLP，CNN，RNN，attention，..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 梯度下降\n",
    "> * 最简单的迭代求解算法\n",
    "> * 选取开始点$x_0$\n",
    "> * 对$t=1,...,T$\n",
    ">> * $x_t=x_{t-1}-\\eta\\nabla f(x_{t-1})$\n",
    "> * $\\eta$叫做学习率\n",
    "> ![](../images/Gradient-descent.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 随机梯度下降\n",
    "> * 有$n$个样本时，计算$f(x)=\\frac{1}{n}\\sum_{i=0}^n\\ell_i(x)$的导数太贵\n",
    "> * 随机梯度下降在时间$t$随机选项样本$t_i$来近似$f(x)$.$$x_t=x_{t-1}-\\eta\\nabla\\ell_{t_i}(x_{t-1})$$$$\\mathbb{E}[\\nabla\\ell_{t_i}(x)]=\\mathbb{E}[\\nabla f(x)]$$\n",
    "> ![](../images/Stochastic-gradient-descent.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小批量随机梯度下降\n",
    "> * 计算单样本的梯度难完全利用硬件资源\n",
    "> * 小批量随机梯度下降在时间$t$采样一个随机子集$I_t\\subset\\{1,...,n\\}$使得$|I_t|=b$,$$x_t=x_{t-1}-\\frac{\\eta_t}{b}\\sum_{i\\in I_t}\\nabla\\ell_i(x_{t-1})$$\n",
    "> * 同样，这是一个无偏的近似，但降低了方差$$\\mathbb{E}[\\frac{1}{b}\\sum_{i\\in I_t}\\nabla\\ell_i(x)]=\\nabla f(x)$$\n",
    "> ![](../images/Mini-batch-stochastic-gradient-descent.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 冲量法\n",
    "> * 冲量法使用平滑过的梯度对权重更新$$g_t=\\frac{1}{b}\\sum_{i\\in I_t}\\nabla\\ell_i(x_{t-1})$$$$v_t=\\beta v_{t-1}+g_t\\quad w_t=w_{t-1}-\\eta v_t$$$$梯度平滑：v_t=g_t+\\beta g_{t-1}+\\beta^2 g_{t-1}+\\beta^3 g_{t-1}+...$$\n",
    "> * $\\beta$常见取值$[0.5, 0.9,0.95,0.99]$\n",
    "> ![](../images/Norm.png)![](../images/Momentum.png)\n",
    "> 左图是正常梯度下降图，右图使用了冲量法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adam\n",
    "> * 记录$v_t=\\beta_1v_{t-1}+(1-\\beta_1)g_t$通常$\\beta_1=0.9$\n",
    "> * 展开$v_t=(1-\\beta_1)(g_t+\\beta_1g_{t-1}+\\beta_1^2g_{t-2}+\\beta_1^3g_{t-3}+...)$\n",
    "> * 因为$\\sum_{i=0}^{\\infty}\\beta_1^i=\\frac{1}{1-\\beta_1}$，所以权重和为1\n",
    "> * 由于$v_0=0$，且$\\sum_{i=0}^t\\beta_1^i=\\frac{1-\\beta_1^t}{1-\\beta_1}$，修正$\\hat{v}_t=\\frac{v_t}{1-\\beta_1^t}$\n",
    "> * 类似记录$s_t=\\beta_2 s_{t-1}+(1-\\beta_2)g_t^2$，通常$\\beta_2=0.999$，且修正$\\hat{s}_t=\\frac{s_t}{1-\\beta_2^t}$\n",
    "> * 计算重新调整后的梯度$g_t'=\\frac{\\hat{v}_t}{\\sqrt{\\hat{s}_t}+\\epsilon}$\n",
    "> * 最后更新$w_t=w_{t-1}-\\eta g_t'$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 总结\n",
    "> * 深度学习模型大多是非凸\n",
    "> * 小批量随机梯度下降是最常用的优化算法\n",
    "> * 冲量对梯度做平滑\n",
    "> * Adam对梯度做平滑，且对梯度各个维度值做重新调整"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}